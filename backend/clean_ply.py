"""
clean_ply.py — Denoise Gaussian Splat PLY files.

Removes three categories of noise common in 3DGS reconstructions:
  1. Low-opacity splats     (nearly transparent → visual noise)
  2. Oversized splats       (floaters with huge scale values)
  3. Statistical outliers   (isolated points far from neighbours)

Usage:
    python clean_ply.py                              # clean all PLY files in results/
    python clean_ply.py results/abc123/baked_splat.ply  # clean a specific file
    python clean_ply.py --opacity 0.1 --scale 0.5    # custom thresholds

The original files are backed up as *_noisy.ply before overwriting.
"""
from __future__ import annotations

import argparse
import struct
import sys
from pathlib import Path

import numpy as np
from scipy.spatial import cKDTree  # type: ignore


# ── PLY I/O ──────────────────────────────────────────────────────────────────

def parse_ply(path: str | Path) -> tuple[dict[str, np.ndarray], list[tuple[str, str]]]:
    """
    Parse a binary little-endian PLY into a dict of named arrays.

    Returns:
        (data, schema)
        data:   {property_name: np.array of shape [N]}
        schema: [(name, dtype_str), …] preserving header order
    """
    path = Path(path)
    with open(path, "rb") as f:
        # ── Read header ──
        header_lines: list[str] = []
        while True:
            line = f.readline().decode("ascii").strip()
            header_lines.append(line)
            if line == "end_header":
                break

        vertex_count = 0
        schema: list[tuple[str, str]] = []  # (name, ply_type)
        for line in header_lines:
            if line.startswith("element vertex"):
                vertex_count = int(line.split()[-1])
            elif line.startswith("property"):
                parts = line.split()
                ply_type = parts[1]   # "float" or "uchar"
                name = parts[2]
                schema.append((name, ply_type))

        # ── Build numpy structured dtype ──
        dtype_map = {"float": np.float32, "uchar": np.uint8}
        np_dtype = np.dtype([(name, dtype_map[t]) for name, t in schema])

        # ── Read binary payload ──
        raw = np.frombuffer(f.read(vertex_count * np_dtype.itemsize), dtype=np_dtype)

    data = {name: raw[name].copy() for name, _ in schema}
    return data, schema


def write_ply(path: str | Path, data: dict[str, np.ndarray],
              schema: list[tuple[str, str]], count: int) -> None:
    """Write a binary little-endian PLY file matching the original schema."""
    path = Path(path)
    with open(path, "wb") as f:
        f.write(b"ply\n")
        f.write(b"format binary_little_endian 1.0\n")
        f.write(b"comment Generated by Nerf4Dgsplat (cleaned)\n")
        f.write(f"element vertex {count}\n".encode())
        for name, ply_type in schema:
            f.write(f"property {ply_type} {name}\n".encode())
        f.write(b"end_header\n")

        dtype_map = {"float": np.float32, "uchar": np.uint8}
        np_dtype = np.dtype([(name, dtype_map[t]) for name, t in schema])
        structured = np.empty(count, dtype=np_dtype)
        for name, _ in schema:
            structured[name] = data[name]
        f.write(structured.tobytes())


# ── Filtering ────────────────────────────────────────────────────────────────

def sigmoid(x: np.ndarray) -> np.ndarray:
    return 1.0 / (1.0 + np.exp(-x))


def clean_splat(
    data: dict[str, np.ndarray],
    schema: list[tuple[str, str]],
    *,
    opacity_threshold: float = 0.05,
    scale_threshold: float = 0.5,
    outlier_neighbours: int = 16,
    outlier_std_ratio: float = 3.0,
    verbose: bool = True,
) -> tuple[dict[str, np.ndarray], int]:
    """
    Apply three cleaning passes to a Gaussian Splat point cloud.

    Args:
        data:               Dict of property arrays from parse_ply().
        schema:             Schema list from parse_ply().
        opacity_threshold:  Remove splats with sigmoid(opacity) below this (0–1).
        scale_threshold:    Remove splats where any exp(log_scale) exceeds this.
        outlier_neighbours: Number of neighbours for statistical outlier removal.
        outlier_std_ratio:  Points with mean-knn-distance > mean + ratio*std are outliers.
        verbose:            Print per-step stats.

    Returns:
        (cleaned_data, count)
    """
    n = len(data["x"])
    mask = np.ones(n, dtype=bool)

    # ── 1) Opacity filtering ──
    if "opacity" in data:
        opa = sigmoid(data["opacity"].astype(np.float64))
        opa_mask = opa >= opacity_threshold
        removed = n - int(opa_mask.sum())
        if verbose:
            print(f"  [opacity]  removed {removed:>8,} / {n:,}  (threshold={opacity_threshold})")
        mask &= opa_mask

    # ── 2) Scale filtering ──
    has_scales = all(f"scale_{i}" in data for i in range(3))
    if has_scales:
        scales = np.stack([data[f"scale_{i}"] for i in range(3)], axis=-1)
        actual_scales = np.exp(scales.astype(np.float64))
        max_scale = actual_scales.max(axis=-1)
        scale_mask = max_scale <= scale_threshold
        removed = int(mask.sum()) - int((mask & scale_mask).sum())
        if verbose:
            print(f"  [scale]    removed {removed:>8,} / {int(mask.sum()):,}  (threshold={scale_threshold})")
        mask &= scale_mask

    # ── 3) Statistical outlier removal ──
    positions = np.stack([data["x"], data["y"], data["z"]], axis=-1)[mask]
    if positions.shape[0] > outlier_neighbours + 1:
        tree = cKDTree(positions)
        dists, _ = tree.query(positions, k=outlier_neighbours + 1)
        mean_dists = dists[:, 1:].mean(axis=1)  # skip self (dist=0)

        global_mean = mean_dists.mean()
        global_std = mean_dists.std()
        threshold = global_mean + outlier_std_ratio * global_std

        inlier_sub = mean_dists <= threshold
        removed = int((~inlier_sub).sum())
        if verbose:
            print(f"  [outlier]  removed {removed:>8,} / {positions.shape[0]:,}  "
                  f"(k={outlier_neighbours}, σ={outlier_std_ratio})")

        # Map sub-mask back to full mask
        full_indices = np.where(mask)[0]
        outlier_indices = full_indices[~inlier_sub]
        mask[outlier_indices] = False

    final_count = int(mask.sum())
    total_removed = n - final_count
    if verbose:
        pct = (total_removed / n * 100) if n > 0 else 0
        print(f"  ── total:  {total_removed:,} removed ({pct:.1f}%)  →  {final_count:,} remaining")

    # Apply mask
    cleaned = {k: v[mask] for k, v in data.items()}
    return cleaned, final_count


# ── Main ─────────────────────────────────────────────────────────────────────

def clean_file(
    filepath: str | Path,
    *,
    opacity_threshold: float = 0.05,
    scale_threshold: float = 0.5,
    outlier_neighbours: int = 16,
    outlier_std_ratio: float = 3.0,
    backup: bool = True,
) -> int:
    """Clean a single PLY file in-place. Returns the final vertex count."""
    filepath = Path(filepath)
    print(f"\n{'─'*60}")
    print(f"Cleaning: {filepath}")

    data, schema = parse_ply(filepath)
    n_original = len(data["x"])
    print(f"  Original: {n_original:,} Gaussians ({filepath.stat().st_size / 1e6:.1f} MB)")

    cleaned, count = clean_splat(
        data, schema,
        opacity_threshold=opacity_threshold,
        scale_threshold=scale_threshold,
        outlier_neighbours=outlier_neighbours,
        outlier_std_ratio=outlier_std_ratio,
    )

    if backup:
        backup_path = filepath.with_name(filepath.stem + "_noisy" + filepath.suffix)
        filepath.rename(backup_path)
        print(f"  Backup:  {backup_path.name}")

    write_ply(filepath, cleaned, schema, count)
    new_size = filepath.stat().st_size / 1e6
    print(f"  ✅ Saved: {count:,} Gaussians ({new_size:.1f} MB)")
    return count


def main():
    parser = argparse.ArgumentParser(description="Denoise Gaussian Splat PLY files")
    parser.add_argument("files", nargs="*",
                        help="PLY file paths (default: all in results/)")
    parser.add_argument("--opacity", type=float, default=0.05,
                        help="Min sigmoid opacity to keep (default: 0.05)")
    parser.add_argument("--scale", type=float, default=0.5,
                        help="Max exp(log_scale) to keep (default: 0.5)")
    parser.add_argument("--neighbours", type=int, default=16,
                        help="k for statistical outlier removal (default: 16)")
    parser.add_argument("--std-ratio", type=float, default=3.0,
                        help="Std dev multiplier for outlier removal (default: 3.0)")
    parser.add_argument("--no-backup", action="store_true",
                        help="Don't save backup of original files")
    args = parser.parse_args()

    if args.files:
        ply_files = [Path(f) for f in args.files]
    else:
        results_dir = Path(__file__).parent / "results"
        ply_files = sorted(results_dir.glob("*/baked_splat.ply"))
        if not ply_files:
            print("No PLY files found in results/")
            sys.exit(1)
        print(f"Found {len(ply_files)} PLY file(s) in results/")

    for f in ply_files:
        if not f.exists():
            print(f"⚠️  Skipping (not found): {f}")
            continue
        clean_file(
            f,
            opacity_threshold=args.opacity,
            scale_threshold=args.scale,
            outlier_neighbours=args.neighbours,
            outlier_std_ratio=args.std_ratio,
            backup=not args.no_backup,
        )

    print(f"\n{'─'*60}")
    print("Done ✅")


if __name__ == "__main__":
    main()
